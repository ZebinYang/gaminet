{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PACKAGE_PARENT = '..'\n",
    "sys.path.append(PACKAGE_PARENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:36:13.711297Z",
     "start_time": "2021-07-02T06:36:09.967047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "from gaminet import GAMINet\n",
    "from gaminet.utils import local_visualize\n",
    "from gaminet.utils import global_visualize_density\n",
    "from gaminet.utils import global_visualize_wo_density\n",
    "from gaminet.utils import feature_importance_visualize\n",
    "from gaminet.utils import plot_regularization\n",
    "from gaminet.utils import plot_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:36:14.152208Z",
     "start_time": "2021-07-02T06:36:13.713194Z"
    }
   },
   "outputs": [],
   "source": [
    "task_type = \"Classification\"\n",
    "\n",
    "data = pd.read_csv(\"./bank.csv\", sep=\";\")\n",
    "meta_info = json.load(open(\"./data_types.json\"))\n",
    "data['month'] = data['month'].replace('jan', 1).replace('feb', 2).replace('mar', 3).replace('apr', 4).\\\n",
    "                              replace('may', 5).replace('jun', 6).replace('jul', 7).replace('aug', 8).\\\n",
    "                              replace('sep', 9).replace('oct', 10).replace('nov', 11).replace('dec', 12)\n",
    "x, y = data.iloc[:,:-1].values, data.iloc[:,[-1]].values\n",
    "xx = np.zeros((x.shape[0], x.shape[1]), dtype=np.float32)\n",
    "for i, (key, item) in enumerate(meta_info.items()):\n",
    "    if item['type'] == 'target':\n",
    "        enc = OrdinalEncoder()\n",
    "        enc.fit(y)\n",
    "        y = enc.transform(y)\n",
    "        meta_info[key]['values'] = enc.categories_[0].tolist()\n",
    "    elif item['type'] == 'categorical':\n",
    "        enc = OrdinalEncoder()\n",
    "        xx[:,[i]] = enc.fit_transform(x[:,[i]])\n",
    "        meta_info[key]['values'] = []\n",
    "        for item in enc.categories_[0].tolist():\n",
    "            try:\n",
    "                if item == int(item):\n",
    "                    meta_info[key]['values'].append(str(int(item)))\n",
    "                else:\n",
    "                    meta_info[key]['values'].append(str(item))\n",
    "            except ValueError:\n",
    "                meta_info[key]['values'].append(str(item))\n",
    "    else:\n",
    "        sx = MinMaxScaler((0, 1))\n",
    "        xx[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "        meta_info[key]['scaler'] = sx\n",
    "train_x, test_x, train_y, test_y = train_test_split(xx.astype(np.float32), y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:36:14.160158Z",
     "start_time": "2021-07-02T06:36:14.155133Z"
    }
   },
   "outputs": [],
   "source": [
    "def metric_wrapper(metric, scaler):\n",
    "    def wrapper(label, pred):\n",
    "        return metric(label, pred, scaler=scaler)\n",
    "    return wrapper\n",
    "\n",
    "def auc(label, pred, scaler=None):\n",
    "    return roc_auc_score(label, pred)\n",
    "\n",
    "get_metric = metric_wrapper(auc, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:19.790287Z",
     "start_time": "2021-07-02T06:36:14.162635Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.35058, val loss: 0.34717\n",
      "Main effects training epoch: 2, train loss: 0.33487, val loss: 0.33082\n",
      "Main effects training epoch: 3, train loss: 0.31394, val loss: 0.30984\n",
      "Main effects training epoch: 4, train loss: 0.29304, val loss: 0.28919\n",
      "Main effects training epoch: 5, train loss: 0.27602, val loss: 0.27282\n",
      "Main effects training epoch: 6, train loss: 0.26421, val loss: 0.26122\n",
      "Main effects training epoch: 7, train loss: 0.25449, val loss: 0.25210\n",
      "Main effects training epoch: 8, train loss: 0.24848, val loss: 0.24627\n",
      "Main effects training epoch: 9, train loss: 0.24289, val loss: 0.24108\n",
      "Main effects training epoch: 10, train loss: 0.24015, val loss: 0.23892\n",
      "Main effects training epoch: 11, train loss: 0.23687, val loss: 0.23550\n",
      "Main effects training epoch: 12, train loss: 0.23457, val loss: 0.23350\n",
      "Main effects training epoch: 13, train loss: 0.23365, val loss: 0.23287\n",
      "Main effects training epoch: 14, train loss: 0.23421, val loss: 0.23310\n",
      "Main effects training epoch: 15, train loss: 0.23096, val loss: 0.23034\n",
      "Main effects training epoch: 16, train loss: 0.23216, val loss: 0.23196\n",
      "Main effects training epoch: 17, train loss: 0.22965, val loss: 0.22937\n",
      "Main effects training epoch: 18, train loss: 0.22933, val loss: 0.22924\n",
      "Main effects training epoch: 19, train loss: 0.22853, val loss: 0.22834\n",
      "Main effects training epoch: 20, train loss: 0.22851, val loss: 0.22838\n",
      "Main effects training epoch: 21, train loss: 0.22762, val loss: 0.22758\n",
      "Main effects training epoch: 22, train loss: 0.22740, val loss: 0.22762\n",
      "Main effects training epoch: 23, train loss: 0.22872, val loss: 0.22869\n",
      "Main effects training epoch: 24, train loss: 0.22687, val loss: 0.22707\n",
      "Main effects training epoch: 25, train loss: 0.22670, val loss: 0.22690\n",
      "Main effects training epoch: 26, train loss: 0.22745, val loss: 0.22766\n",
      "Main effects training epoch: 27, train loss: 0.22606, val loss: 0.22660\n",
      "Main effects training epoch: 28, train loss: 0.22565, val loss: 0.22620\n",
      "Main effects training epoch: 29, train loss: 0.22545, val loss: 0.22604\n",
      "Main effects training epoch: 30, train loss: 0.22523, val loss: 0.22577\n",
      "Main effects training epoch: 31, train loss: 0.22511, val loss: 0.22576\n",
      "Main effects training epoch: 32, train loss: 0.22528, val loss: 0.22597\n",
      "Main effects training epoch: 33, train loss: 0.22522, val loss: 0.22603\n",
      "Main effects training epoch: 34, train loss: 0.22491, val loss: 0.22582\n",
      "Main effects training epoch: 35, train loss: 0.22459, val loss: 0.22532\n",
      "Main effects training epoch: 36, train loss: 0.22441, val loss: 0.22536\n",
      "Main effects training epoch: 37, train loss: 0.22547, val loss: 0.22619\n",
      "Main effects training epoch: 38, train loss: 0.22460, val loss: 0.22566\n",
      "Main effects training epoch: 39, train loss: 0.22480, val loss: 0.22556\n",
      "Main effects training epoch: 40, train loss: 0.22430, val loss: 0.22541\n",
      "Main effects training epoch: 41, train loss: 0.22405, val loss: 0.22505\n",
      "Main effects training epoch: 42, train loss: 0.22418, val loss: 0.22513\n",
      "Main effects training epoch: 43, train loss: 0.22403, val loss: 0.22508\n",
      "Main effects training epoch: 44, train loss: 0.22412, val loss: 0.22502\n",
      "Main effects training epoch: 45, train loss: 0.22371, val loss: 0.22487\n",
      "Main effects training epoch: 46, train loss: 0.22414, val loss: 0.22514\n",
      "Main effects training epoch: 47, train loss: 0.22389, val loss: 0.22505\n",
      "Main effects training epoch: 48, train loss: 0.22374, val loss: 0.22512\n",
      "Main effects training epoch: 49, train loss: 0.22375, val loss: 0.22492\n",
      "Main effects training epoch: 50, train loss: 0.22365, val loss: 0.22480\n",
      "Main effects training epoch: 51, train loss: 0.22378, val loss: 0.22511\n",
      "Main effects training epoch: 52, train loss: 0.22337, val loss: 0.22470\n",
      "Main effects training epoch: 53, train loss: 0.22356, val loss: 0.22499\n",
      "Main effects training epoch: 54, train loss: 0.22415, val loss: 0.22563\n",
      "Main effects training epoch: 55, train loss: 0.22327, val loss: 0.22468\n",
      "Main effects training epoch: 56, train loss: 0.22334, val loss: 0.22477\n",
      "Main effects training epoch: 57, train loss: 0.22331, val loss: 0.22477\n",
      "Main effects training epoch: 58, train loss: 0.22314, val loss: 0.22471\n",
      "Main effects training epoch: 59, train loss: 0.22317, val loss: 0.22459\n",
      "Main effects training epoch: 60, train loss: 0.22371, val loss: 0.22515\n",
      "Main effects training epoch: 61, train loss: 0.22334, val loss: 0.22489\n",
      "Main effects training epoch: 62, train loss: 0.22308, val loss: 0.22465\n",
      "Main effects training epoch: 63, train loss: 0.22312, val loss: 0.22478\n",
      "Main effects training epoch: 64, train loss: 0.22310, val loss: 0.22487\n",
      "Main effects training epoch: 65, train loss: 0.22323, val loss: 0.22487\n",
      "Main effects training epoch: 66, train loss: 0.22304, val loss: 0.22471\n",
      "Main effects training epoch: 67, train loss: 0.22291, val loss: 0.22467\n",
      "Main effects training epoch: 68, train loss: 0.22301, val loss: 0.22480\n",
      "Main effects training epoch: 69, train loss: 0.22290, val loss: 0.22464\n",
      "Main effects training epoch: 70, train loss: 0.22290, val loss: 0.22462\n",
      "Main effects training epoch: 71, train loss: 0.22299, val loss: 0.22483\n",
      "Main effects training epoch: 72, train loss: 0.22301, val loss: 0.22470\n",
      "Main effects training epoch: 73, train loss: 0.22286, val loss: 0.22472\n",
      "Main effects training epoch: 74, train loss: 0.22317, val loss: 0.22528\n",
      "Main effects training epoch: 75, train loss: 0.22283, val loss: 0.22478\n",
      "Main effects training epoch: 76, train loss: 0.22277, val loss: 0.22465\n",
      "Main effects training epoch: 77, train loss: 0.22399, val loss: 0.22568\n",
      "Main effects training epoch: 78, train loss: 0.22305, val loss: 0.22500\n",
      "Main effects training epoch: 79, train loss: 0.22271, val loss: 0.22449\n",
      "Main effects training epoch: 80, train loss: 0.22264, val loss: 0.22459\n",
      "Main effects training epoch: 81, train loss: 0.22359, val loss: 0.22556\n",
      "Main effects training epoch: 82, train loss: 0.22330, val loss: 0.22517\n",
      "Main effects training epoch: 83, train loss: 0.22257, val loss: 0.22463\n",
      "Main effects training epoch: 84, train loss: 0.22311, val loss: 0.22517\n",
      "Main effects training epoch: 85, train loss: 0.22258, val loss: 0.22477\n",
      "Main effects training epoch: 86, train loss: 0.22256, val loss: 0.22479\n",
      "Main effects training epoch: 87, train loss: 0.22253, val loss: 0.22454\n",
      "Main effects training epoch: 88, train loss: 0.22231, val loss: 0.22452\n",
      "Main effects training epoch: 89, train loss: 0.22253, val loss: 0.22451\n",
      "Main effects training epoch: 90, train loss: 0.22243, val loss: 0.22468\n",
      "Main effects training epoch: 91, train loss: 0.22240, val loss: 0.22470\n",
      "Main effects training epoch: 92, train loss: 0.22248, val loss: 0.22485\n",
      "Main effects training epoch: 93, train loss: 0.22372, val loss: 0.22567\n",
      "Main effects training epoch: 94, train loss: 0.22473, val loss: 0.22721\n",
      "Main effects training epoch: 95, train loss: 0.22331, val loss: 0.22530\n",
      "Main effects training epoch: 96, train loss: 0.22214, val loss: 0.22433\n",
      "Main effects training epoch: 97, train loss: 0.22399, val loss: 0.22646\n",
      "Main effects training epoch: 98, train loss: 0.22253, val loss: 0.22487\n",
      "Main effects training epoch: 99, train loss: 0.22200, val loss: 0.22430\n",
      "Main effects training epoch: 100, train loss: 0.22260, val loss: 0.22491\n",
      "Main effects training epoch: 101, train loss: 0.22242, val loss: 0.22468\n",
      "Main effects training epoch: 102, train loss: 0.22199, val loss: 0.22433\n",
      "Main effects training epoch: 103, train loss: 0.22213, val loss: 0.22470\n",
      "Main effects training epoch: 104, train loss: 0.22290, val loss: 0.22524\n",
      "Main effects training epoch: 105, train loss: 0.22296, val loss: 0.22553\n",
      "Main effects training epoch: 106, train loss: 0.22191, val loss: 0.22435\n",
      "Main effects training epoch: 107, train loss: 0.22254, val loss: 0.22485\n",
      "Main effects training epoch: 108, train loss: 0.22264, val loss: 0.22518\n",
      "Main effects training epoch: 109, train loss: 0.22188, val loss: 0.22442\n",
      "Main effects training epoch: 110, train loss: 0.22177, val loss: 0.22407\n",
      "Main effects training epoch: 111, train loss: 0.22203, val loss: 0.22463\n",
      "Main effects training epoch: 112, train loss: 0.22172, val loss: 0.22428\n",
      "Main effects training epoch: 113, train loss: 0.22547, val loss: 0.22826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 114, train loss: 0.22170, val loss: 0.22422\n",
      "Main effects training epoch: 115, train loss: 0.22171, val loss: 0.22431\n",
      "Main effects training epoch: 116, train loss: 0.22243, val loss: 0.22509\n",
      "Main effects training epoch: 117, train loss: 0.22174, val loss: 0.22409\n",
      "Main effects training epoch: 118, train loss: 0.22185, val loss: 0.22421\n",
      "Main effects training epoch: 119, train loss: 0.22164, val loss: 0.22419\n",
      "Main effects training epoch: 120, train loss: 0.22238, val loss: 0.22487\n",
      "Main effects training epoch: 121, train loss: 0.22268, val loss: 0.22513\n",
      "Main effects training epoch: 122, train loss: 0.22201, val loss: 0.22465\n",
      "Main effects training epoch: 123, train loss: 0.22266, val loss: 0.22552\n",
      "Main effects training epoch: 124, train loss: 0.22174, val loss: 0.22440\n",
      "Main effects training epoch: 125, train loss: 0.22191, val loss: 0.22458\n",
      "Main effects training epoch: 126, train loss: 0.22190, val loss: 0.22453\n",
      "Main effects training epoch: 127, train loss: 0.22207, val loss: 0.22487\n",
      "Main effects training epoch: 128, train loss: 0.22211, val loss: 0.22457\n",
      "Main effects training epoch: 129, train loss: 0.22161, val loss: 0.22411\n",
      "Main effects training epoch: 130, train loss: 0.22145, val loss: 0.22421\n",
      "Main effects training epoch: 131, train loss: 0.22146, val loss: 0.22403\n",
      "Main effects training epoch: 132, train loss: 0.22337, val loss: 0.22620\n",
      "Main effects training epoch: 133, train loss: 0.22166, val loss: 0.22438\n",
      "Main effects training epoch: 134, train loss: 0.22187, val loss: 0.22473\n",
      "Main effects training epoch: 135, train loss: 0.22145, val loss: 0.22405\n",
      "Main effects training epoch: 136, train loss: 0.22188, val loss: 0.22458\n",
      "Main effects training epoch: 137, train loss: 0.22203, val loss: 0.22461\n",
      "Main effects training epoch: 138, train loss: 0.22220, val loss: 0.22504\n",
      "Main effects training epoch: 139, train loss: 0.22135, val loss: 0.22406\n",
      "Main effects training epoch: 140, train loss: 0.22158, val loss: 0.22425\n",
      "Main effects training epoch: 141, train loss: 0.22143, val loss: 0.22402\n",
      "Main effects training epoch: 142, train loss: 0.22146, val loss: 0.22415\n",
      "Main effects training epoch: 143, train loss: 0.22140, val loss: 0.22404\n",
      "Main effects training epoch: 144, train loss: 0.22141, val loss: 0.22423\n",
      "Main effects training epoch: 145, train loss: 0.22507, val loss: 0.22757\n",
      "Main effects training epoch: 146, train loss: 0.22158, val loss: 0.22440\n",
      "Main effects training epoch: 147, train loss: 0.22166, val loss: 0.22455\n",
      "Main effects training epoch: 148, train loss: 0.22168, val loss: 0.22422\n",
      "Main effects training epoch: 149, train loss: 0.22132, val loss: 0.22416\n",
      "Main effects training epoch: 150, train loss: 0.22138, val loss: 0.22398\n",
      "Main effects training epoch: 151, train loss: 0.22122, val loss: 0.22410\n",
      "Main effects training epoch: 152, train loss: 0.22114, val loss: 0.22415\n",
      "Main effects training epoch: 153, train loss: 0.22152, val loss: 0.22453\n",
      "Main effects training epoch: 154, train loss: 0.22226, val loss: 0.22508\n",
      "Main effects training epoch: 155, train loss: 0.22117, val loss: 0.22398\n",
      "Main effects training epoch: 156, train loss: 0.22163, val loss: 0.22450\n",
      "Main effects training epoch: 157, train loss: 0.22134, val loss: 0.22430\n",
      "Main effects training epoch: 158, train loss: 0.22125, val loss: 0.22413\n",
      "Main effects training epoch: 159, train loss: 0.22144, val loss: 0.22439\n",
      "Main effects training epoch: 160, train loss: 0.22107, val loss: 0.22397\n",
      "Main effects training epoch: 161, train loss: 0.22131, val loss: 0.22413\n",
      "Main effects training epoch: 162, train loss: 0.22134, val loss: 0.22443\n",
      "Main effects training epoch: 163, train loss: 0.22107, val loss: 0.22400\n",
      "Main effects training epoch: 164, train loss: 0.22114, val loss: 0.22407\n",
      "Main effects training epoch: 165, train loss: 0.22096, val loss: 0.22386\n",
      "Main effects training epoch: 166, train loss: 0.22105, val loss: 0.22386\n",
      "Main effects training epoch: 167, train loss: 0.22092, val loss: 0.22399\n",
      "Main effects training epoch: 168, train loss: 0.22101, val loss: 0.22406\n",
      "Main effects training epoch: 169, train loss: 0.22117, val loss: 0.22414\n",
      "Main effects training epoch: 170, train loss: 0.22116, val loss: 0.22396\n",
      "Main effects training epoch: 171, train loss: 0.22115, val loss: 0.22419\n",
      "Main effects training epoch: 172, train loss: 0.22127, val loss: 0.22422\n",
      "Main effects training epoch: 173, train loss: 0.22106, val loss: 0.22403\n",
      "Main effects training epoch: 174, train loss: 0.22120, val loss: 0.22390\n",
      "Main effects training epoch: 175, train loss: 0.22249, val loss: 0.22577\n",
      "Main effects training epoch: 176, train loss: 0.22097, val loss: 0.22403\n",
      "Main effects training epoch: 177, train loss: 0.22141, val loss: 0.22429\n",
      "Main effects training epoch: 178, train loss: 0.22206, val loss: 0.22507\n",
      "Main effects training epoch: 179, train loss: 0.22240, val loss: 0.22520\n",
      "Main effects training epoch: 180, train loss: 0.22087, val loss: 0.22380\n",
      "Main effects training epoch: 181, train loss: 0.22138, val loss: 0.22440\n",
      "Main effects training epoch: 182, train loss: 0.22128, val loss: 0.22419\n",
      "Main effects training epoch: 183, train loss: 0.22099, val loss: 0.22388\n",
      "Main effects training epoch: 184, train loss: 0.22124, val loss: 0.22463\n",
      "Main effects training epoch: 185, train loss: 0.22096, val loss: 0.22405\n",
      "Main effects training epoch: 186, train loss: 0.22083, val loss: 0.22386\n",
      "Main effects training epoch: 187, train loss: 0.22124, val loss: 0.22452\n",
      "Main effects training epoch: 188, train loss: 0.22163, val loss: 0.22441\n",
      "Main effects training epoch: 189, train loss: 0.22193, val loss: 0.22467\n",
      "Main effects training epoch: 190, train loss: 0.22186, val loss: 0.22483\n",
      "Main effects training epoch: 191, train loss: 0.22121, val loss: 0.22417\n",
      "Main effects training epoch: 192, train loss: 0.22074, val loss: 0.22387\n",
      "Main effects training epoch: 193, train loss: 0.22110, val loss: 0.22398\n",
      "Main effects training epoch: 194, train loss: 0.22090, val loss: 0.22419\n",
      "Main effects training epoch: 195, train loss: 0.22334, val loss: 0.22646\n",
      "Main effects training epoch: 196, train loss: 0.22093, val loss: 0.22390\n",
      "Main effects training epoch: 197, train loss: 0.22163, val loss: 0.22460\n",
      "Main effects training epoch: 198, train loss: 0.22078, val loss: 0.22391\n",
      "Main effects training epoch: 199, train loss: 0.22075, val loss: 0.22407\n",
      "Main effects training epoch: 200, train loss: 0.22083, val loss: 0.22398\n",
      "Main effects training epoch: 201, train loss: 0.22156, val loss: 0.22455\n",
      "Main effects training epoch: 202, train loss: 0.22086, val loss: 0.22437\n",
      "Main effects training epoch: 203, train loss: 0.22110, val loss: 0.22432\n",
      "Main effects training epoch: 204, train loss: 0.22147, val loss: 0.22492\n",
      "Main effects training epoch: 205, train loss: 0.22151, val loss: 0.22449\n",
      "Main effects training epoch: 206, train loss: 0.22164, val loss: 0.22493\n",
      "Main effects training epoch: 207, train loss: 0.22133, val loss: 0.22488\n",
      "Main effects training epoch: 208, train loss: 0.22072, val loss: 0.22404\n",
      "Main effects training epoch: 209, train loss: 0.22112, val loss: 0.22435\n",
      "Main effects training epoch: 210, train loss: 0.22072, val loss: 0.22408\n",
      "Main effects training epoch: 211, train loss: 0.22082, val loss: 0.22429\n",
      "Main effects training epoch: 212, train loss: 0.22560, val loss: 0.22946\n",
      "Main effects training epoch: 213, train loss: 0.22069, val loss: 0.22401\n",
      "Main effects training epoch: 214, train loss: 0.22078, val loss: 0.22401\n",
      "Main effects training epoch: 215, train loss: 0.22091, val loss: 0.22426\n",
      "Main effects training epoch: 216, train loss: 0.22059, val loss: 0.22407\n",
      "Main effects training epoch: 217, train loss: 0.22192, val loss: 0.22535\n",
      "Main effects training epoch: 218, train loss: 0.22059, val loss: 0.22398\n",
      "Main effects training epoch: 219, train loss: 0.22267, val loss: 0.22568\n",
      "Main effects training epoch: 220, train loss: 0.22136, val loss: 0.22465\n",
      "Main effects training epoch: 221, train loss: 0.22216, val loss: 0.22587\n",
      "Main effects training epoch: 222, train loss: 0.22159, val loss: 0.22487\n",
      "Main effects training epoch: 223, train loss: 0.22066, val loss: 0.22408\n",
      "Main effects training epoch: 224, train loss: 0.22074, val loss: 0.22441\n",
      "Main effects training epoch: 225, train loss: 0.22086, val loss: 0.22456\n",
      "Main effects training epoch: 226, train loss: 0.22163, val loss: 0.22530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 227, train loss: 0.22057, val loss: 0.22400\n",
      "Main effects training epoch: 228, train loss: 0.22087, val loss: 0.22414\n",
      "Main effects training epoch: 229, train loss: 0.22063, val loss: 0.22399\n",
      "Main effects training epoch: 230, train loss: 0.22111, val loss: 0.22465\n",
      "Main effects training epoch: 231, train loss: 0.22088, val loss: 0.22421\n",
      "Early stop at epoch 231, with validation loss: 0.22421\n",
      "##########Stage 1: main effect training stop.##########\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.22202, val loss: 0.22394\n",
      "Interaction training epoch: 2, train loss: 0.22183, val loss: 0.22384\n",
      "Interaction training epoch: 3, train loss: 0.22170, val loss: 0.22373\n",
      "Interaction training epoch: 4, train loss: 0.22155, val loss: 0.22361\n",
      "Interaction training epoch: 5, train loss: 0.22136, val loss: 0.22339\n",
      "Interaction training epoch: 6, train loss: 0.22097, val loss: 0.22306\n",
      "Interaction training epoch: 7, train loss: 0.22039, val loss: 0.22261\n",
      "Interaction training epoch: 8, train loss: 0.21963, val loss: 0.22194\n",
      "Interaction training epoch: 9, train loss: 0.21876, val loss: 0.22106\n",
      "Interaction training epoch: 10, train loss: 0.21788, val loss: 0.22057\n",
      "Interaction training epoch: 11, train loss: 0.21692, val loss: 0.21973\n",
      "Interaction training epoch: 12, train loss: 0.21620, val loss: 0.21909\n",
      "Interaction training epoch: 13, train loss: 0.21556, val loss: 0.21849\n",
      "Interaction training epoch: 14, train loss: 0.21480, val loss: 0.21794\n",
      "Interaction training epoch: 15, train loss: 0.21425, val loss: 0.21760\n",
      "Interaction training epoch: 16, train loss: 0.21398, val loss: 0.21770\n",
      "Interaction training epoch: 17, train loss: 0.21320, val loss: 0.21669\n",
      "Interaction training epoch: 18, train loss: 0.21279, val loss: 0.21666\n",
      "Interaction training epoch: 19, train loss: 0.21247, val loss: 0.21654\n",
      "Interaction training epoch: 20, train loss: 0.21220, val loss: 0.21628\n",
      "Interaction training epoch: 21, train loss: 0.21193, val loss: 0.21622\n",
      "Interaction training epoch: 22, train loss: 0.21172, val loss: 0.21561\n",
      "Interaction training epoch: 23, train loss: 0.21140, val loss: 0.21563\n",
      "Interaction training epoch: 24, train loss: 0.21124, val loss: 0.21525\n",
      "Interaction training epoch: 25, train loss: 0.21116, val loss: 0.21498\n",
      "Interaction training epoch: 26, train loss: 0.21090, val loss: 0.21501\n",
      "Interaction training epoch: 27, train loss: 0.21092, val loss: 0.21531\n",
      "Interaction training epoch: 28, train loss: 0.21070, val loss: 0.21513\n",
      "Interaction training epoch: 29, train loss: 0.21063, val loss: 0.21484\n",
      "Interaction training epoch: 30, train loss: 0.21055, val loss: 0.21502\n",
      "Interaction training epoch: 31, train loss: 0.21037, val loss: 0.21499\n",
      "Interaction training epoch: 32, train loss: 0.21031, val loss: 0.21462\n",
      "Interaction training epoch: 33, train loss: 0.21034, val loss: 0.21522\n",
      "Interaction training epoch: 34, train loss: 0.21017, val loss: 0.21461\n",
      "Interaction training epoch: 35, train loss: 0.21007, val loss: 0.21457\n",
      "Interaction training epoch: 36, train loss: 0.20991, val loss: 0.21431\n",
      "Interaction training epoch: 37, train loss: 0.20982, val loss: 0.21400\n",
      "Interaction training epoch: 38, train loss: 0.20976, val loss: 0.21452\n",
      "Interaction training epoch: 39, train loss: 0.20958, val loss: 0.21406\n",
      "Interaction training epoch: 40, train loss: 0.20954, val loss: 0.21441\n",
      "Interaction training epoch: 41, train loss: 0.20947, val loss: 0.21439\n",
      "Interaction training epoch: 42, train loss: 0.20942, val loss: 0.21384\n",
      "Interaction training epoch: 43, train loss: 0.20936, val loss: 0.21378\n",
      "Interaction training epoch: 44, train loss: 0.20975, val loss: 0.21471\n",
      "Interaction training epoch: 45, train loss: 0.20936, val loss: 0.21419\n",
      "Interaction training epoch: 46, train loss: 0.20907, val loss: 0.21390\n",
      "Interaction training epoch: 47, train loss: 0.20905, val loss: 0.21410\n",
      "Interaction training epoch: 48, train loss: 0.20904, val loss: 0.21391\n",
      "Interaction training epoch: 49, train loss: 0.20904, val loss: 0.21416\n",
      "Interaction training epoch: 50, train loss: 0.20883, val loss: 0.21353\n",
      "Interaction training epoch: 51, train loss: 0.20877, val loss: 0.21413\n",
      "Interaction training epoch: 52, train loss: 0.20871, val loss: 0.21411\n",
      "Interaction training epoch: 53, train loss: 0.20871, val loss: 0.21356\n",
      "Interaction training epoch: 54, train loss: 0.20870, val loss: 0.21394\n",
      "Interaction training epoch: 55, train loss: 0.20863, val loss: 0.21403\n",
      "Interaction training epoch: 56, train loss: 0.20903, val loss: 0.21474\n",
      "Interaction training epoch: 57, train loss: 0.20888, val loss: 0.21380\n",
      "Interaction training epoch: 58, train loss: 0.20839, val loss: 0.21385\n",
      "Interaction training epoch: 59, train loss: 0.20844, val loss: 0.21380\n",
      "Interaction training epoch: 60, train loss: 0.20837, val loss: 0.21396\n",
      "Interaction training epoch: 61, train loss: 0.20847, val loss: 0.21341\n",
      "Interaction training epoch: 62, train loss: 0.20833, val loss: 0.21348\n",
      "Interaction training epoch: 63, train loss: 0.20827, val loss: 0.21372\n",
      "Interaction training epoch: 64, train loss: 0.20822, val loss: 0.21374\n",
      "Interaction training epoch: 65, train loss: 0.20821, val loss: 0.21397\n",
      "Interaction training epoch: 66, train loss: 0.20821, val loss: 0.21381\n",
      "Interaction training epoch: 67, train loss: 0.20811, val loss: 0.21386\n",
      "Interaction training epoch: 68, train loss: 0.20809, val loss: 0.21358\n",
      "Interaction training epoch: 69, train loss: 0.20829, val loss: 0.21356\n",
      "Interaction training epoch: 70, train loss: 0.20801, val loss: 0.21359\n",
      "Interaction training epoch: 71, train loss: 0.20803, val loss: 0.21344\n",
      "Interaction training epoch: 72, train loss: 0.20798, val loss: 0.21362\n",
      "Interaction training epoch: 73, train loss: 0.20809, val loss: 0.21371\n",
      "Interaction training epoch: 74, train loss: 0.20794, val loss: 0.21323\n",
      "Interaction training epoch: 75, train loss: 0.20807, val loss: 0.21369\n",
      "Interaction training epoch: 76, train loss: 0.20802, val loss: 0.21375\n",
      "Interaction training epoch: 77, train loss: 0.20784, val loss: 0.21335\n",
      "Interaction training epoch: 78, train loss: 0.20791, val loss: 0.21351\n",
      "Interaction training epoch: 79, train loss: 0.20790, val loss: 0.21348\n",
      "Interaction training epoch: 80, train loss: 0.20778, val loss: 0.21337\n",
      "Interaction training epoch: 81, train loss: 0.20776, val loss: 0.21322\n",
      "Interaction training epoch: 82, train loss: 0.20791, val loss: 0.21289\n",
      "Interaction training epoch: 83, train loss: 0.20808, val loss: 0.21404\n",
      "Interaction training epoch: 84, train loss: 0.20775, val loss: 0.21345\n",
      "Interaction training epoch: 85, train loss: 0.20777, val loss: 0.21315\n",
      "Interaction training epoch: 86, train loss: 0.20765, val loss: 0.21327\n",
      "Interaction training epoch: 87, train loss: 0.20768, val loss: 0.21350\n",
      "Interaction training epoch: 88, train loss: 0.20767, val loss: 0.21329\n",
      "Interaction training epoch: 89, train loss: 0.20759, val loss: 0.21338\n",
      "Interaction training epoch: 90, train loss: 0.20768, val loss: 0.21346\n",
      "Interaction training epoch: 91, train loss: 0.20764, val loss: 0.21338\n",
      "Interaction training epoch: 92, train loss: 0.20756, val loss: 0.21298\n",
      "Interaction training epoch: 93, train loss: 0.20757, val loss: 0.21328\n",
      "Interaction training epoch: 94, train loss: 0.20769, val loss: 0.21355\n",
      "Interaction training epoch: 95, train loss: 0.20757, val loss: 0.21347\n",
      "Interaction training epoch: 96, train loss: 0.20755, val loss: 0.21309\n",
      "Interaction training epoch: 97, train loss: 0.20751, val loss: 0.21335\n",
      "Interaction training epoch: 98, train loss: 0.20768, val loss: 0.21315\n",
      "Interaction training epoch: 99, train loss: 0.20745, val loss: 0.21316\n",
      "Interaction training epoch: 100, train loss: 0.20755, val loss: 0.21307\n",
      "Interaction training epoch: 101, train loss: 0.20753, val loss: 0.21337\n",
      "Interaction training epoch: 102, train loss: 0.20760, val loss: 0.21344\n",
      "Interaction training epoch: 103, train loss: 0.20848, val loss: 0.21474\n",
      "Interaction training epoch: 104, train loss: 0.20737, val loss: 0.21295\n",
      "Interaction training epoch: 105, train loss: 0.20738, val loss: 0.21318\n",
      "Interaction training epoch: 106, train loss: 0.20735, val loss: 0.21341\n",
      "Interaction training epoch: 107, train loss: 0.20747, val loss: 0.21374\n",
      "Interaction training epoch: 108, train loss: 0.20737, val loss: 0.21329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 109, train loss: 0.20732, val loss: 0.21323\n",
      "Interaction training epoch: 110, train loss: 0.20730, val loss: 0.21333\n",
      "Interaction training epoch: 111, train loss: 0.20727, val loss: 0.21317\n",
      "Interaction training epoch: 112, train loss: 0.20735, val loss: 0.21299\n",
      "Interaction training epoch: 113, train loss: 0.20719, val loss: 0.21305\n",
      "Interaction training epoch: 114, train loss: 0.20717, val loss: 0.21314\n",
      "Interaction training epoch: 115, train loss: 0.20721, val loss: 0.21308\n",
      "Interaction training epoch: 116, train loss: 0.20726, val loss: 0.21329\n",
      "Interaction training epoch: 117, train loss: 0.20728, val loss: 0.21302\n",
      "Interaction training epoch: 118, train loss: 0.20717, val loss: 0.21300\n",
      "Interaction training epoch: 119, train loss: 0.20734, val loss: 0.21344\n",
      "Interaction training epoch: 120, train loss: 0.20724, val loss: 0.21310\n",
      "Interaction training epoch: 121, train loss: 0.20717, val loss: 0.21293\n",
      "Interaction training epoch: 122, train loss: 0.20717, val loss: 0.21373\n",
      "Interaction training epoch: 123, train loss: 0.20733, val loss: 0.21324\n",
      "Interaction training epoch: 124, train loss: 0.20715, val loss: 0.21281\n",
      "Interaction training epoch: 125, train loss: 0.20717, val loss: 0.21269\n",
      "Interaction training epoch: 126, train loss: 0.20722, val loss: 0.21325\n",
      "Interaction training epoch: 127, train loss: 0.20724, val loss: 0.21332\n",
      "Interaction training epoch: 128, train loss: 0.20714, val loss: 0.21320\n",
      "Interaction training epoch: 129, train loss: 0.20717, val loss: 0.21296\n",
      "Interaction training epoch: 130, train loss: 0.20715, val loss: 0.21302\n",
      "Interaction training epoch: 131, train loss: 0.20713, val loss: 0.21299\n",
      "Interaction training epoch: 132, train loss: 0.20734, val loss: 0.21330\n",
      "Interaction training epoch: 133, train loss: 0.20729, val loss: 0.21309\n",
      "Interaction training epoch: 134, train loss: 0.20721, val loss: 0.21276\n",
      "Interaction training epoch: 135, train loss: 0.20701, val loss: 0.21311\n",
      "Interaction training epoch: 136, train loss: 0.20728, val loss: 0.21288\n",
      "Interaction training epoch: 137, train loss: 0.20712, val loss: 0.21287\n",
      "Interaction training epoch: 138, train loss: 0.20707, val loss: 0.21361\n",
      "Interaction training epoch: 139, train loss: 0.20704, val loss: 0.21290\n",
      "Interaction training epoch: 140, train loss: 0.20702, val loss: 0.21315\n",
      "Interaction training epoch: 141, train loss: 0.20736, val loss: 0.21358\n",
      "Interaction training epoch: 142, train loss: 0.20695, val loss: 0.21294\n",
      "Interaction training epoch: 143, train loss: 0.20702, val loss: 0.21336\n",
      "Interaction training epoch: 144, train loss: 0.20781, val loss: 0.21444\n",
      "Interaction training epoch: 145, train loss: 0.20708, val loss: 0.21285\n",
      "Interaction training epoch: 146, train loss: 0.20744, val loss: 0.21394\n",
      "Interaction training epoch: 147, train loss: 0.20691, val loss: 0.21305\n",
      "Interaction training epoch: 148, train loss: 0.20688, val loss: 0.21294\n",
      "Interaction training epoch: 149, train loss: 0.20739, val loss: 0.21378\n",
      "Interaction training epoch: 150, train loss: 0.20683, val loss: 0.21303\n",
      "Interaction training epoch: 151, train loss: 0.20699, val loss: 0.21293\n",
      "Interaction training epoch: 152, train loss: 0.20686, val loss: 0.21263\n",
      "Interaction training epoch: 153, train loss: 0.20682, val loss: 0.21264\n",
      "Interaction training epoch: 154, train loss: 0.20692, val loss: 0.21244\n",
      "Interaction training epoch: 155, train loss: 0.20678, val loss: 0.21279\n",
      "Interaction training epoch: 156, train loss: 0.20734, val loss: 0.21382\n",
      "Interaction training epoch: 157, train loss: 0.20683, val loss: 0.21262\n",
      "Interaction training epoch: 158, train loss: 0.20678, val loss: 0.21255\n",
      "Interaction training epoch: 159, train loss: 0.20674, val loss: 0.21306\n",
      "Interaction training epoch: 160, train loss: 0.20679, val loss: 0.21312\n",
      "Interaction training epoch: 161, train loss: 0.20701, val loss: 0.21265\n",
      "Interaction training epoch: 162, train loss: 0.20701, val loss: 0.21325\n",
      "Interaction training epoch: 163, train loss: 0.20679, val loss: 0.21305\n",
      "Interaction training epoch: 164, train loss: 0.20669, val loss: 0.21268\n",
      "Interaction training epoch: 165, train loss: 0.20674, val loss: 0.21233\n",
      "Interaction training epoch: 166, train loss: 0.20671, val loss: 0.21260\n",
      "Interaction training epoch: 167, train loss: 0.20677, val loss: 0.21263\n",
      "Interaction training epoch: 168, train loss: 0.20660, val loss: 0.21258\n",
      "Interaction training epoch: 169, train loss: 0.20691, val loss: 0.21313\n",
      "Interaction training epoch: 170, train loss: 0.20665, val loss: 0.21256\n",
      "Interaction training epoch: 171, train loss: 0.20671, val loss: 0.21267\n",
      "Interaction training epoch: 172, train loss: 0.20687, val loss: 0.21301\n",
      "Interaction training epoch: 173, train loss: 0.20665, val loss: 0.21249\n",
      "Interaction training epoch: 174, train loss: 0.20733, val loss: 0.21349\n",
      "Interaction training epoch: 175, train loss: 0.20652, val loss: 0.21214\n",
      "Interaction training epoch: 176, train loss: 0.20668, val loss: 0.21257\n",
      "Interaction training epoch: 177, train loss: 0.20645, val loss: 0.21253\n",
      "Interaction training epoch: 178, train loss: 0.20660, val loss: 0.21244\n",
      "Interaction training epoch: 179, train loss: 0.20682, val loss: 0.21242\n",
      "Interaction training epoch: 180, train loss: 0.20663, val loss: 0.21285\n",
      "Interaction training epoch: 181, train loss: 0.20653, val loss: 0.21234\n",
      "Interaction training epoch: 182, train loss: 0.20680, val loss: 0.21334\n",
      "Interaction training epoch: 183, train loss: 0.20654, val loss: 0.21280\n"
     ]
    }
   ],
   "source": [
    "folder = \"./results/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "model_bank = GAMINet(meta_info=meta_info, interact_num=20,\n",
    "            interact_arch=[40] * 1, subnet_arch=[40] * 2, \n",
    "            batch_size=200, task_type=task_type, activation_func=tf.nn.relu, \n",
    "            main_effect_epochs=5000, interaction_epochs=5000, tuning_epochs=500, \n",
    "            lr_bp=[0.001, 0.001, 0.001], early_stop_thres=[50, 50, 50],\n",
    "            heredity=True, loss_threshold=0.01, reg_clarity=0.1,\n",
    "            mono_increasing_list=[11],\n",
    "            mono_decreasing_list=[0], \n",
    "            lattice_size=10,\n",
    "            verbose=True, val_ratio=0.2, random_state=0)\n",
    "model_bank.fit(train_x, train_y, sample_weight=np.random.uniform(0, 1, size=(train_x.shape[0], 1)))\n",
    "data_dict_logs = model_bank.summary_logs(save_dict=False)\n",
    "plot_trajectory(data_dict_logs, folder=folder, name=\"bank_traj\", save_png=True, save_eps=True)\n",
    "plot_regularization(data_dict_logs, folder=folder, name=\"bank_regu\", save_png=True, save_eps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:25.794067Z",
     "start_time": "2021-07-02T06:54:19.792098Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dict_global = model_bank.global_explain(save_dict=True, folder=folder, name=\"bank_global\")\n",
    "global_visualize_density(data_dict_global, folder=folder, name=\"bank_global\",\n",
    "                         main_effect_num=8, interaction_num=4, cols_per_row=4, save_png=True, save_eps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:25.971320Z",
     "start_time": "2021-07-02T06:54:25.795768Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_visualize(data_dict_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the prediction of a test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:28.652098Z",
     "start_time": "2021-07-02T06:54:25.972863Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dict_local = model_bank.local_explain(test_x[[0]], test_y[[0]], save_dict=False)\n",
    "local_visualize(data_dict_local[0], save_png=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save and load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:28.846055Z",
     "start_time": "2021-07-02T06:54:28.654757Z"
    }
   },
   "outputs": [],
   "source": [
    "model_bank.save(folder=\"./\", name=\"model_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:29.269244Z",
     "start_time": "2021-07-02T06:54:28.847929Z"
    }
   },
   "outputs": [],
   "source": [
    "## The reloaded model should not be refit again\n",
    "modelnew = GAMINet(meta_info={})\n",
    "modelnew.load(folder=\"./\", name=\"model_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:54:34.634821Z",
     "start_time": "2021-07-02T06:54:29.270847Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train = modelnew.predict(train_x)\n",
    "pred_test = modelnew.predict(test_x)\n",
    "gaminet_stat = np.hstack([np.round(get_metric(train_y, pred_train),5), \n",
    "                      np.round(get_metric(test_y, pred_test),5)])\n",
    "print(gaminet_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
